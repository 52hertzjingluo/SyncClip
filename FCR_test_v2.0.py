"""
@brief: è§£è¯´è§†é¢‘é‡ç»„ v2.0
@authors: Huang YanZhe (Author)
@version: 2.0

@features :
    - å®Œæ•´é›†æˆ ChatterboxTTS æ¨¡å—ã€‚å½“æ£€æµ‹åˆ°è§£è¯´è§†é¢‘æ—¶ï¼Œä½¿ç”¨æ„è¯‘åçš„æ–‡ç¨¿ï¼Œå¹¶ä¸¥æ ¼æŒ‰ç…§åŸå§‹æ—¶é—´æˆ³ç”Ÿæˆå…¨æ–°çš„TTSé…éŸ³éŸ³è½¨ã€‚
    - â€œæ¶æ„ï¼šåŸºäºé•œå¤´æ£€æµ‹çš„éçº¿æ€§é‡ç»„åŒ¹é…â€ä½œä¸ºæ ¸å¿ƒå¼•æ“ï¼Œæ³¨å…¥åˆ°åŸæœ‰çš„è§†è§‰åŒ¹é…æµç¨‹ä¸­ã€‚
    - æ„è¯‘åŠŸèƒ½ç°åœ¨ä¼šåŒæ—¶è¿”å›â€œåŸå§‹æ–‡ç¨¿â€å’Œâ€œæ„è¯‘æ–‡ç¨¿â€ä¸¤ä»½å¸¦æ—¶é—´æˆ³çš„æ•°æ®ï¼Œæ ¼å¼ç»Ÿä¸€ã€‚
    - æ‰€æœ‰åŸå§‹åŠŸèƒ½å‡è¢«å¢å¼ºï¼šCSVæ‰¹é‡å¤„ç†ã€å¢å¼ºçš„éŸ³é¢‘åˆ†æã€å¢å¼ºçš„ç«–å±è’™ç‰ˆè£åˆ‡ã€‚
    - mainå‡½æ•°ä¸­æä¾›äº†æ¸…æ™°çš„å…¨å±€é…ç½®åŒºï¼Œå¯ç»Ÿä¸€è®¾ç½®åŒ…æ‹¬TTSåœ¨å†…çš„å„é¡¹å…³é”®å‚æ•°ã€‚
"""
import cv2
import torch
import numpy as np
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    CLIPProcessor, CLIPModel, Dinov2Model, AutoImageProcessor as DinoImageProcessor
)
from PIL import Image
import subprocess
import os
import logging
from datetime import datetime
import json
from sklearn.metrics.pairwise import cosine_similarity
import whisper
import re
from typing import List, Dict, Tuple, Optional
import time
import traceback
import warnings
import pickle
import hashlib
import pandas as pd
import librosa
import soundfile as sf
from scipy.spatial.distance import cosine as cosine_distance
from scenedetect import detect, ContentDetector, SceneManager

# å¯¼å…¥TTSç›¸å…³åº“
import torchaudio as ta
from chatterbox.tts import ChatterboxTTS

warnings.filterwarnings('ignore')
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# è‡ªå®šä¹‰JSONç¼–ç å™¨
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer): return int(obj)
        if isinstance(obj, np.floating): return float(obj)
        if isinstance(obj, np.bool_): return bool(obj)
        if isinstance(obj, np.ndarray): return obj.tolist()
        return super(NumpyEncoder, self).default(obj)

# ParaphraseGenerator ç±»
class ParaphraseGenerator:
    def __init__(self, device):
        self.device = device
        self.model = None
        self.tokenizer = None
    def load(self):
        if self.model is None:
            model_name = "humarin/chatgpt_paraphraser_on_T5_base"
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)
    def paraphrase_text(self, text: str, num_return_sequences: int = 1) -> List[str]:
        if not text or not text.strip(): return [""]
        if not self.model: self.load()
        input_ids = self.tokenizer(f'paraphrase: {text}', return_tensors="pt", padding="longest", max_length=128, truncation=True).input_ids.to(self.device)
        outputs = self.model.generate(input_ids, temperature=1.0, repetition_penalty=10.0, num_return_sequences=num_return_sequences, no_repeat_ngram_size=2, num_beams=5, num_beam_groups=1, max_length=128)
        return self.tokenizer.batch_decode(outputs, skip_special_tokens=True)

# AudioAnalyzer ç±»
class AudioAnalyzer:
    def __init__(self, logger):
        self.logger = logger
    def extract_audio_features(self, audio_path: str) -> Optional[Dict]:
        try:
            y, sr = librosa.load(audio_path, sr=16000)
            if len(y) == 0: return None
            mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
            f0, _, _ = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)
            rms = librosa.feature.rms(y=y)
            return {'mfcc_mean': np.mean(mfccs, axis=1), 'mfcc_std': np.std(mfccs, axis=1),'f0_mean': np.nanmean(f0), 'f0_std': np.nanstd(f0),'spectral_centroid_mean': np.mean(spectral_centroid), 'spectral_bandwidth_mean': np.mean(spectral_bandwidth),'rms_mean': np.mean(rms)}
        except Exception as e:
            self.logger.error(f"æå–éŸ³é¢‘ç‰¹å¾å¤±è´¥ from {audio_path}: {e}"); return None
    def compare_audio_similarity(self, features1: Dict, features2: Dict) -> float:
        if not features1 or not features2: return 0.0
        weights = {'mfcc': 0.5, 'f0': 0.2, 'spectral': 0.2, 'energy': 0.1}
        total_similarity = 0.0
        try:
            mfcc_vec1 = np.nan_to_num(np.concatenate([features1['mfcc_mean'], features1['mfcc_std']]))
            mfcc_vec2 = np.nan_to_num(np.concatenate([features2['mfcc_mean'], features2['mfcc_std']]))
            total_similarity += (1 - cosine_distance(mfcc_vec1, mfcc_vec2)) * weights['mfcc']
            f0_mean_dist = abs(features1['f0_mean'] - features2['f0_mean']) / max(features1['f0_mean'], features2['f0_mean'], 1)
            total_similarity += (1 - np.nan_to_num(f0_mean_dist)) * weights['f0']
            spec_vec1 = np.nan_to_num([features1['spectral_centroid_mean'], features1['spectral_bandwidth_mean']])
            spec_vec2 = np.nan_to_num([features2['spectral_centroid_mean'], features2['spectral_bandwidth_mean']])
            total_similarity += (1 - cosine_distance(spec_vec1, spec_vec2)) * weights['spectral']
            rms_dist = abs(features1['rms_mean'] - features2['rms_mean']) / max(features1['rms_mean'], features2['rms_mean'], 1e-6)
            total_similarity += (1 - np.nan_to_num(rms_dist)) * weights['energy']
            return max(0.0, float(total_similarity))
        except Exception as e:
            self.logger.error(f"æ¯”è¾ƒéŸ³é¢‘ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}"); return 0.0
    def is_narration(self, youtube_audio: str, movie_audio: str, threshold: float = 0.7) -> Tuple[bool, float]:
        self.logger.info("å¼€å§‹è¿›è¡ŒéŸ³è‰²ç›¸ä¼¼åº¦åˆ†æ...")
        youtube_features = self.extract_audio_features(youtube_audio)
        movie_features = self.extract_audio_features(movie_audio)
        if not youtube_features or not movie_features:
            self.logger.warning("æ— æ³•æå–ä»»ä¸€éŸ³é¢‘ç‰¹å¾ï¼Œé»˜è®¤åˆ¤æ–­ä¸ºè§£è¯´"); return True, 0.0
        similarity = self.compare_audio_similarity(youtube_features, movie_features)
        is_narration_flag = similarity < threshold
        self.logger.info(f"éŸ³é¢‘ç›¸ä¼¼åº¦: {similarity:.3f} (é˜ˆå€¼: {threshold}). åˆ¤æ–­ä¸º: {'è§£è¯´éŸ³é¢‘' if is_narration_flag else 'åŸç‰‡éŸ³é¢‘'}")
        return is_narration_flag, similarity

class YouTubeMovieMatcher:
    def __init__(self, output_dir: str, log_level: str = "INFO"):
        self.output_dir = output_dir
        self.logger = self.setup_logging(log_level)
        self.device = self.get_device()
        self.models = {}; self.processors = {}
        self.resources_to_cleanup = []
        self.cache_dir = self.ensure_dir(os.path.join(self.output_dir, "cache"))
        
    def setup_logging(self, log_level: str) -> logging.Logger:
        logger = logging.getLogger(self.__class__.__name__); logger.setLevel(getattr(logging, log_level.upper()))
        if logger.handlers: logger.handlers.clear()
        console_handler = logging.StreamHandler(); console_handler.setLevel(logging.INFO)
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'); console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        log_file = os.path.join(self.output_dir, "logs", f"matcher_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        file_handler = logging.FileHandler(log_file, encoding='utf-8'); file_handler.setLevel(logging.DEBUG)
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(funcName)s] - %(message)s'); file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler); logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—æ–‡ä»¶ï¼š{log_file}"); return logger
    
    def get_device(self) -> torch.device:
        try:
            if torch.cuda.is_available(): device = torch.device("cuda"); self.logger.info(f"ä½¿ç”¨CUDA GPU: {torch.cuda.get_device_name(0)}")
            elif torch.backends.mps.is_available(): device = torch.device("mps"); self.logger.info("ä½¿ç”¨ Apple Metal (MPS)")
            else: device = torch.device("cpu"); self.logger.info("ä½¿ç”¨CPU")
            return device
        except Exception as e: self.logger.error(f"è·å–è®¾å¤‡å¤±è´¥: {e}"); return torch.device("cpu")

    def cleanup_resources(self):
        self.logger.info("æ¸…ç†ä¸´æ—¶èµ„æº..."); import shutil
        for resource in self.resources_to_cleanup:
            try:
                if os.path.exists(resource):
                    if os.path.isfile(resource): os.remove(resource)
                    else: shutil.rmtree(resource)
                    self.logger.debug(f"å·²åˆ é™¤èµ„æº: {resource}")
            except Exception as e: self.logger.error(f"åˆ é™¤èµ„æºå¤±è´¥ {resource}: {e}")
        self.resources_to_cleanup = []
    
    def add_resource_for_cleanup(self, resource: str):
        if resource and resource not in self.resources_to_cleanup: self.resources_to_cleanup.append(resource)

    def ensure_dir(self, path: str) -> Optional[str]:
        try: os.makedirs(path, exist_ok=True); return path
        except Exception as e: self.logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}"); return None
    
    def get_file_hash(self, file_path: str) -> Optional[str]:
        if not os.path.exists(file_path): return None
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""): hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e: self.logger.error(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}"); return None
    
    def save_features(self, features: np.ndarray, video_path: str, feature_type: str):
        try:
            video_hash = self.get_file_hash(video_path)
            if not video_hash: video_hash = os.path.basename(video_path).replace('.', '_')
            features_path = os.path.join(self.cache_dir, f"{feature_type}_{video_hash}.npy")
            np.save(features_path, features); self.logger.info(f"{feature_type} ç‰¹å¾å·²ä¿å­˜åˆ°ç¼“å­˜: {features_path}"); return True
        except Exception as e: self.logger.error(f"ä¿å­˜{feature_type}ç‰¹å¾å¤±è´¥: {e}"); return False
    
    def load_features(self, video_path: str, feature_type: str) -> Optional[np.ndarray]:
        try:
            video_hash = self.get_file_hash(video_path)
            if not video_hash: video_hash = os.path.basename(video_path).replace('.', '_')
            features_path = os.path.join(self.cache_dir, f"{feature_type}_{video_hash}.npy")
            if not os.path.exists(features_path): return None
            features = np.load(features_path); self.logger.info(f"ä»ç¼“å­˜åŠ è½½{feature_type}ç‰¹å¾æˆåŠŸ: {features_path}"); return features
        except Exception as e: self.logger.error(f"åŠ è½½{feature_type}ç‰¹å¾å¤±è´¥: {e}"); return None
    
    def load_model(self, model_key: str):
        if model_key in self.models: return
        self.logger.info(f"å¼€å§‹åŠ è½½æ¨¡å‹: {model_key}...")
        try:
            if model_key == 'clip':
                model_name = "openai/clip-vit-large-patch14"; self.processors['clip'] = CLIPProcessor.from_pretrained(model_name); self.models['clip'] = CLIPModel.from_pretrained(model_name).to(self.device).eval()
            elif model_key == 'dinov2':
                model_name = "facebook/dinov2-large"; self.processors['dinov2'] = DinoImageProcessor.from_pretrained(model_name); self.models['dinov2'] = Dinov2Model.from_pretrained(model_name).to(self.device).eval()
            elif model_key == 'whisper':
                model_name = "base"; self.models['whisper'] = whisper.load_model(model_name, device=self.device)
            elif model_key == 'tts':
                self.logger.info("åŠ è½½ ChatterboxTTS æ¨¡å‹ (é¦–æ¬¡åŠ è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´)...")
                self.models['tts'] = ChatterboxTTS.from_pretrained(device=self.device)
            else: raise ValueError(f"æœªçŸ¥çš„æ¨¡å‹é”®: {model_key}")
            self.logger.info(f"æ¨¡å‹ {model_key} åŠ è½½æˆåŠŸ")
        except Exception as e: self.logger.error(f"åŠ è½½æ¨¡å‹ {model_key} å¤±è´¥: {e}"); raise

    def detect_video_orientation(self, video_path: str) -> Dict:
        self.logger.info(f"æ£€æµ‹è§†é¢‘æ–¹å‘: {os.path.basename(video_path)}")
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened(): self.logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}"); return None
            width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps, total_frames = cap.get(cv2.CAP_PROP_FPS), int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            aspect_ratio = width / height if height > 0 else 0
            video_info = {'path': video_path, 'width': width, 'height': height, 'aspect_ratio': aspect_ratio,'fps': fps, 'total_frames': total_frames,'is_portrait': aspect_ratio < 1.0, 'content_region': None}
            if video_info['is_portrait']:
                self.logger.info(f"æ£€æµ‹åˆ°ç«–å±è§†é¢‘ ({width}x{height})ï¼Œåˆ†æå†…å®¹åŒºåŸŸ...")
                content_region = self.detect_content_region(cap, width, height)
                video_info['content_region'] = content_region
                if content_region: self.logger.info(f"æ£€æµ‹åˆ°å†…å®¹åŒºåŸŸ: x={content_region['x']}, y={content_region['y']}, w={content_region['width']}, h={content_region['height']}")
            cap.release()
            return video_info
        except Exception as e: self.logger.error(f"æ£€æµ‹è§†é¢‘æ–¹å‘å¤±è´¥: {e}"); return None

    def detect_content_region(self, cap, width: int, height: int, sample_frames: int = 20) -> Optional[Dict]:
        self.logger.debug("ä½¿ç”¨å¢å¼ºçš„å½¢æ€å­¦ç®—æ³•æ£€æµ‹å†…å®¹åŒºåŸŸ...")
        try:
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            frame_indices = np.linspace(0, total_frames - 1, sample_frames, dtype=int)
            combined_mask = np.zeros((height, width), dtype=np.uint8)
            for idx in frame_indices:
                cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
                ret, frame = cap.read()
                if not ret: continue
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                _, mask = cv2.threshold(gray, 20, 255, cv2.THRESH_BINARY)
                combined_mask = cv2.bitwise_or(combined_mask, mask)
            kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (int(width*0.05), int(height*0.01)))
            closed_mask = cv2.morphologyEx(combined_mask, cv2.MORPH_CLOSE, kernel, iterations=3)
            opened_mask = cv2.morphologyEx(closed_mask, cv2.MORPH_OPEN, kernel, iterations=2)
            contours, _ = cv2.findContours(opened_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            if not contours: self.logger.warning("åœ¨è’™ç‰ˆä¸­æœªæ‰¾åˆ°ä»»ä½•è½®å»“"); return None
            largest_contour = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest_contour)
            if w < width * 0.6 or h < height * 0.4: self.logger.warning(f"æ£€æµ‹åˆ°çš„å†…å®¹åŒºåŸŸè¿‡å°(w:{w}, h:{h})ï¼Œå¯èƒ½ä¸å‡†ç¡®ï¼Œæ”¾å¼ƒè£å‰ªã€‚"); return None
            return {'x': x, 'y': y, 'width': w, 'height': h}
        except Exception as e: self.logger.error(f"æ£€æµ‹å†…å®¹åŒºåŸŸæ—¶å‡ºé”™: {e}"); return None

    def preprocess_video(self, video_path: str, video_info: Dict, output_prefix: str) -> str:
        if not video_info or not video_info.get('content_region'):
            self.logger.info("è§†é¢‘æ— éœ€é¢„å¤„ç†ã€‚")
            return video_path
        processed_dir = self.ensure_dir(os.path.join(self.output_dir, "temp_preprocessed"))
        processed_path = os.path.join(processed_dir, f"{output_prefix}_processed.mp4")
        if os.path.exists(processed_path):
            self.logger.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„é¢„å¤„ç†è§†é¢‘: {processed_path}"); self.add_resource_for_cleanup(processed_dir); return processed_path
        region = video_info['content_region']
        crop_filter = f"crop={region['width']}:{region['height']}:{region['x']}:{region['y']}"
        cmd = ["ffmpeg", "-y", "-i", video_path, "-vf", crop_filter, "-c:a", "copy", processed_path]
        self.logger.info(f"æ­£åœ¨æ‰§è¡Œè£å‰ªé¢„å¤„ç†: {crop_filter}...")
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0:
            self.logger.info("è§†é¢‘é¢„å¤„ç†æˆåŠŸã€‚"); self.add_resource_for_cleanup(processed_dir); return processed_path
        else:
            self.logger.error(f"è§†é¢‘é¢„å¤„ç†å¤±è´¥: {result.stderr}"); return video_path

    def download_youtube_video(self, url: str, max_retries: int = 3) -> Optional[Dict]:
        self.logger.info(f"å¼€å§‹ä¸‹è½½YouTubeè§†é¢‘: {url}")
        videos_dir = self.ensure_dir(os.path.join(self.output_dir, "videos"))
        ydl_opts = {'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best', 'outtmpl': os.path.join(videos_dir, '%(title)s.%(ext)s'),}
        for attempt in range(max_retries):
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    info = ydl.extract_info(url, download=True)
                    video_path = ydl.prepare_filename(info)
                    self.add_resource_for_cleanup(video_path)
                    return {'title': info.get('title', 'æœªçŸ¥'), 'duration': info.get('duration', 0), 'video_path': video_path, 'url': url}
            except Exception as e:
                self.logger.error(f"ä¸‹è½½å°è¯• {attempt + 1} å¤±è´¥: {e}")
                if attempt < max_retries - 1: time.sleep(5)
        return None
    
    # æ‰€æœ‰æŠ¥å‘Šç”Ÿæˆå‡½æ•°
    def save_match_report(self, shot_matches: List[Dict], youtube_info: Dict, movie_info: Dict, youtube_transcript: List[Dict], output_path: str):
        self.logger.info(f"ç”ŸæˆåŒ¹é…æŠ¥å‘Š: {output_path}")
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(f"YouTubeè§£è¯´è§†é¢‘ä¸ç”µå½±åŸç‰‡åŒ¹é…æŠ¥å‘Š (æ¶æ„ v6.0)\n{'='*80}\n\n")
                f.write(f"YouTubeè§†é¢‘: {os.path.basename(youtube_info.get('path', 'N/A'))}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write("åŒ¹é…é•œå¤´è¯¦æƒ…\n" + "-"*80 + "\n\n")
                if not shot_matches: f.write("æœªæ‰¾åˆ°ä»»ä½•é«˜ç½®ä¿¡åº¦çš„åŒ¹é…é•œå¤´ã€‚\n")
                for i, match in enumerate(shot_matches):
                    yt_shot, movie_shot = match['yt_shot'], match['movie_shot']
                    yt_start_time = yt_shot['start_frame'] / youtube_info['fps']
                    yt_end_time = yt_shot['end_frame'] / youtube_info['fps']
                    movie_start_time = movie_shot['start_frame'] / movie_info['fps']
                    movie_end_time = movie_shot['end_frame'] / movie_info['fps']
                    f.write(f"åŒ¹é…é•œå¤´ {i+1}:\n")
                    f.write(f"  - YouTubeé•œå¤´ {yt_shot['id']:>2}: [{yt_start_time:7.2f}s - {yt_end_time:7.2f}s]\n")
                    f.write(f"  - ç”µå½±é•œå¤´   {movie_shot['id']:>2}: [{movie_start_time:7.2f}s - {movie_end_time:7.2f}s]\n")
                    f.write(f"  - é•œå¤´ç›¸ä¼¼åº¦: {match['similarity']:.4f}\n")
                    f.write(f"  - å…³è”è§£è¯´:\n")
                    found_transcript = False
                    for transcript in youtube_transcript:
                        if max(transcript['start'], yt_start_time) < min(transcript['end'], yt_end_time):
                            f.write(f"    [{transcript['start']:.2f}s] {transcript['text']}\n"); found_transcript = True
                    if not found_transcript: f.write("    (æ­¤é•œå¤´æ— å¯¹åº”è§£è¯´æ–‡ç¨¿)\n")
                    f.write("\n")
        except Exception as e: self.logger.error(f"ä¿å­˜åŒ¹é…æŠ¥å‘Šå¤±è´¥: {str(e)}")

    def save_quality_report(self, quality_info: Dict, output_path: str):
        self.logger.info(f"ç”Ÿæˆè´¨é‡æŠ¥å‘Š: {output_path}")
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("åŒ¹é…è´¨é‡æŠ¥å‘Š (æ¶æ„ v6.0)\n" + "=" * 80 + "\n\n")
                total_shots = quality_info.get('total_yt_shots', 1); matched_shots = quality_info.get('matched_shots', 0)
                match_ratio = (matched_shots / total_shots) * 100 if total_shots > 0 else 0
                f.write(f"æ€»è§£è¯´é•œå¤´æ•°: {total_shots}\n")
                f.write(f"æˆåŠŸåŒ¹é…é•œå¤´æ•°: {matched_shots} ({match_ratio:.1f}%)\n")
                f.write(f"å¹³å‡é•œå¤´ç›¸ä¼¼åº¦: {quality_info.get('avg_similarity', 0):.4f}\n")
                f.write(f"æ•´ä½“ç½®ä¿¡åº¦(åŸºäºåŒ¹é…ç‡): {quality_info.get('confidence', 0):.1f}%\n\n")
                f.write(f"è´¨é‡è¯„çº§: ")
                confidence = quality_info.get('confidence', 0)
                if confidence >= 90: f.write("â˜…â˜…â˜…â˜…â˜… ä¼˜ç§€\n")
                elif confidence >= 75: f.write("â˜…â˜…â˜…â˜…â˜† è‰¯å¥½\n")
                elif confidence >= 60: f.write("â˜…â˜…â˜…â˜†â˜† ä¸­ç­‰\n")
                else: f.write("â˜…â˜…â˜†â˜†â˜† è¾ƒå·®\n")
                if confidence < 75: f.write("\nå»ºè®®:\n  - å°è¯•é€‚å½“é™ä½ `shot_similarity_threshold` é…ç½®å€¼ã€‚\n  - æ£€æŸ¥ç”µå½±åŸç‰‡ç‰ˆæœ¬æ˜¯å¦ä¸è§£è¯´ç´ æä¸€è‡´ã€‚\n")
        except Exception as e: self.logger.error(f"ä¿å­˜è´¨é‡æŠ¥å‘Šå¤±è´¥: {str(e)}")

    def save_alternative_segments(self, *args, **kwargs):
        self.logger.debug("æ–°æ¶æ„ä¸‹æ— å€™é€‰ç‰‡æ®µæ¦‚å¿µï¼Œè·³è¿‡ä¿å­˜ã€‚")
        pass

class EnhancedYouTubeMovieMatcher(YouTubeMovieMatcher):
    def __init__(self, output_dir: str, log_level: str = "INFO"):
        super().__init__(output_dir, log_level)
        self.audio_analyzer = AudioAnalyzer(self.logger)
        self.paraphrase_generator = ParaphraseGenerator(self.device)
    
    def read_csv_file(self, csv_path: str) -> List[Dict]:
        try:
            self.logger.info(f"è¯»å–CSVæ–‡ä»¶: {csv_path}")
            df = pd.read_csv(csv_path, header=None, names=['youtube_clip', 'movie_path'])
            valid_entries = []
            for idx, row in df.iterrows():
                if not os.path.exists(row['youtube_clip']): self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {row['youtube_clip']}"); continue
                if not os.path.exists(row['movie_path']): self.logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {row['movie_path']}"); continue
                valid_entries.append({'youtube_clip': row['youtube_clip'], 'movie_path': row['movie_path'], 'index': idx})
            self.logger.info(f"å…±è¯»å– {len(valid_entries)} ä¸ªæœ‰æ•ˆæ¡ç›®")
            return valid_entries
        except Exception as e:
            self.logger.error(f"è¯»å–CSVæ–‡ä»¶å¤±è´¥: {e}"); return []
            
    def extract_audio_from_clip(self, video_path: str, output_prefix: str) -> Optional[str]:
        try:
            audio_dir = self.ensure_dir(os.path.join(self.output_dir, "temp_audio"))
            audio_path = os.path.join(audio_dir, f"{output_prefix}_audio.wav")
            cmd = ["ffmpeg", "-y", "-i", video_path, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", audio_path]
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode == 0 and os.path.exists(audio_path):
                self.add_resource_for_cleanup(audio_path)
                return audio_path
            else:
                self.logger.error(f"éŸ³é¢‘æå–å¤±è´¥: {result.stderr}"); return None
        except Exception as e:
            self.logger.error(f"æå–éŸ³é¢‘å¼‚å¸¸: {e}"); return None

    def transcribe_and_paraphrase(self, audio_path: str, output_prefix: str) -> Dict:
        self.logger.info("å¼€å§‹è½¬å½•å¹¶æ„è¯‘éŸ³é¢‘...")
        self.load_model('whisper')
        self.paraphrase_generator.load()
        transcription = self.models['whisper'].transcribe(audio_path, language="zh", word_timestamps=True)
        original_segments, paraphrased_segments = [], []
        for segment in transcription["segments"]:
            original_text = segment["text"].strip()
            original_segments.append({"start": segment["start"], "end": segment["end"], "text": original_text})
            paraphrases = self.paraphrase_generator.paraphrase_text(original_text)
            paraphrased_segments.append({"start": segment["start"], "end": segment["end"], "text": paraphrases[0] if paraphrases else original_text})
        report_dir = self.ensure_dir(os.path.join(self.output_dir, "reports"))
        report_path = os.path.join(report_dir, f"{output_prefix}_paraphrase_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            for orig, para in zip(original_segments, paraphrased_segments):
                f.write(f"[{orig['start']:.2f}s -> {orig['end']:.2f}s]\n  åŸæ–‡: {orig['text']}\n  æ„è¯‘: {para['text']}\n\n")
        return {'original_transcript': original_segments, 'paraphrased_transcript': paraphrased_segments}
        
    def process_single_clip(self, youtube_clip: str, movie_path: str, audio_threshold: float, output_prefix: str) -> Dict:
        self.logger.info(f"\n--- æ­¥éª¤ 2: éŸ³é¢‘åˆ†æä¸æ„è¯‘ ---")
        result = {'is_narration': True, 'audio_similarity': 0.0}
        try:
            youtube_audio = self.extract_audio_from_clip(youtube_clip, f"{output_prefix}_yt")
            movie_audio = self.extract_audio_from_clip(movie_path, f"{output_prefix}_movie")
            if not youtube_audio or not movie_audio: raise IOError("éŸ³é¢‘æå–å¤±è´¥")
            is_narration, similarity = self.audio_analyzer.is_narration(youtube_audio, movie_audio, audio_threshold)
            result['is_narration'], result['audio_similarity'] = is_narration, similarity
            if is_narration:
                self.logger.info("æ£€æµ‹ä¸ºè§£è¯´éŸ³é¢‘ï¼Œè¿›è¡Œè½¬å½•å’Œæ„è¯‘...")
                paraphrase_data = self.transcribe_and_paraphrase(youtube_audio, output_prefix)
                result.update(paraphrase_data)
            else:
                self.logger.info("æ£€æµ‹ä¸ºåŸç‰‡éŸ³é¢‘ï¼Œè·³è¿‡è½¬å½•å’Œæ”¹å†™ã€‚")
            return result
        except Exception as e:
            self.logger.error(f"å¤„ç†éŸ³é¢‘ç‰‡æ®µå¤±è´¥: {e}"); return result

    # æ¶æ„çš„æ ¸å¿ƒå®ç°
    def stage0_generate_feature_bank(self, video_path: str) -> bool:
        self.logger.info(f"===== é˜¶æ®µ 0: ä¸º {os.path.basename(video_path)} ç”Ÿæˆç‰¹å¾é“¶è¡Œ =====")
        if self.load_features(video_path, 'clip') is not None and self.load_features(video_path, 'dino') is not None:
            self.logger.info("æ‰€æœ‰æ¨¡å‹ç‰¹å¾ç¼“å­˜å·²å­˜åœ¨ã€‚"); return True
        self.load_model('clip'); self.load_model('dinov2')
        try:
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened(): raise IOError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)); all_clip, all_dino = [], []
            batch_size = 16
            for start_frame in range(0, total_frames, batch_size):
                frames = []; end_frame = min(start_frame + batch_size, total_frames)
                for i in range(start_frame, end_frame):
                    cap.set(cv2.CAP_PROP_POS_FRAMES, i); ret, frame = cap.read()
                    if ret: frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))
                if not frames: continue
                with torch.no_grad():
                    clip_in = self.processors['clip'](images=frames, return_tensors="pt").to(self.device); clip_feat = self.models['clip'].get_image_features(**clip_in)
                    all_clip.append((clip_feat / clip_feat.norm(dim=-1, keepdim=True)).cpu().numpy())
                    dino_in = self.processors['dinov2'](images=frames, return_tensors="pt").to(self.device); dino_out = self.models['dinov2'](**dino_in).last_hidden_state.mean(dim=1)
                    all_dino.append((dino_out / dino_out.norm(dim=-1, keepdim=True)).cpu().numpy())
                if (start_frame // batch_size) % 10 == 0: self.logger.info(f"ç‰¹å¾é“¶è¡Œç”Ÿæˆè¿›åº¦: {end_frame}/{total_frames}")
            cap.release()
            self.save_features(np.vstack(all_clip), video_path, 'clip'); self.save_features(np.vstack(all_dino), video_path, 'dino')
            return True
        except Exception as e: self.logger.error(f"ç”Ÿæˆç‰¹å¾é“¶è¡Œå¤±è´¥: {e}\n{traceback.format_exc()}"); return False

    def stage1_global_coarse_localization(self, yt_clip_path: str, movie_path: str, sparse_interval_sec: int) -> Optional[List[Dict]]:
        self.logger.info("===== é˜¶æ®µ 1: å…¨å±€ç²—å®šä½ =====")
        try:
            yt_feats = self.load_features(yt_clip_path, 'clip'); movie_feats = self.load_features(movie_path, 'clip')
            if yt_feats is None or movie_feats is None: raise FileNotFoundError("ç‰¹å¾é“¶è¡Œä¸å®Œæ•´")
        except Exception as e: self.logger.error(f"åŠ è½½CLIPç‰¹å¾å¤±è´¥: {e}"); return None
        info_yt = self.detect_video_orientation(yt_clip_path); info_movie = self.detect_video_orientation(movie_path)
        yt_fps = info_yt['fps']; movie_fps = info_movie['fps']
        yt_sfeats = yt_feats[::int(yt_fps*sparse_interval_sec)]; movie_sfeats = movie_feats[::int(movie_fps*sparse_interval_sec)]
        if len(yt_sfeats)==0 or len(movie_sfeats)<len(yt_sfeats): self.logger.error("ç¨€ç–ç‰¹å¾ä¸è¶³"); return None
        sim_mat = cosine_similarity(yt_sfeats, movie_sfeats); win_size = len(yt_sfeats)
        scores = [np.diagonal(sim_mat[:, i:i+win_size]).mean() for i in range(len(movie_sfeats) - win_size + 1)]
        best_idx = np.argmax(scores); max_sim = scores[best_idx]
        start_f = best_idx * int(movie_fps*sparse_interval_sec); end_f = (best_idx+win_size)*int(movie_fps*sparse_interval_sec)
        pad_f = int(movie_fps*60); start_f = max(0, start_f-pad_f); end_f = min(len(movie_feats), end_f+pad_f)
        zone = {'start_frame': start_f, 'end_frame': end_f, 'confidence': float(max_sim)}
        self.logger.info(f"ç²—å®šä½å®Œæˆã€‚å€™é€‰åŒºåŸŸ: å¸§[{start_f}-{end_f}], ç½®ä¿¡åº¦: {max_sim:.3f}"); return [zone]

    def stage2_shot_matching(self, yt_clip_path: str, movie_path: str, candidate_zones: List[Dict], shot_sim_threshold: float) -> Optional[List[Dict]]:
        self.logger.info("===== é˜¶æ®µ 2: é•œå¤´è§£æ„ä¸æ— åºåŒ¹é… =====")
        yt_shots = self._detect_shots(yt_clip_path); movie_shots = []
        for zone in candidate_zones: movie_shots.extend(self._detect_shots(movie_path, start_frame=zone['start_frame'], end_frame=zone['end_frame']))
        if not yt_shots or not movie_shots: self.logger.error("é•œå¤´æ£€æµ‹å¤±è´¥"); return None
        yt_fps = self._generate_shot_fingerprints(yt_clip_path, yt_shots); movie_fps = self._generate_shot_fingerprints(movie_path, movie_shots)
        if not yt_fps or not movie_fps: self.logger.error("æŒ‡çº¹ç”Ÿæˆå¤±è´¥"); return None
        matches = []; movie_fp_mat = np.array([fp['fingerprint'] for fp in movie_fps])
        for yt_fp in yt_fps:
            sims = cosine_similarity([yt_fp['fingerprint']], movie_fp_mat)[0]; best_idx = np.argmax(sims)
            if float(sims[best_idx]) >= shot_sim_threshold:
                matches.append({'yt_shot': yt_fp['shot_info'], 'movie_shot': movie_fps[best_idx]['shot_info'], 'similarity': float(sims[best_idx])})
        self.logger.info(f"é•œå¤´åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(matches)}/{len(yt_shots)} ä¸ªé«˜ç½®ä¿¡åº¦åŒ¹é…ã€‚"); return matches
        
    def _detect_shots(self, video_path: str, start_frame: Optional[int] = None, end_frame: Optional[int] = None) -> List[Dict]:
        """ä½¿ç”¨PySceneDetectæ£€æµ‹é•œå¤´"""
        try:
            from scenedetect import FrameTimecode # ç¡®ä¿FrameTimecodeè¢«å¯¼å…¥
            
            scene_manager = SceneManager()
            scene_manager.add_detector(ContentDetector(threshold=27.0)) # é˜ˆå€¼å¯è°ƒ
            
            # ä¸ºäº†åˆ›å»ºæ­£ç¡®çš„æ—¶é—´ç å¯¹è±¡ï¼Œéœ€è¦è·å–è§†é¢‘çš„FPS
            video_info = self.detect_video_orientation(video_path)
            if not video_info or 'fps' not in video_info:
                self.logger.error(f"æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ä»¥è¿›è¡Œé•œå¤´æ£€æµ‹: {video_path}")
                return []
            fps = video_info['fps']

            start_timecode = None
            end_timecode = None
            
            # æ ¹æ®ä¼ å…¥çš„å¸§å·åˆ›å»ºFrameTimecodeå¯¹è±¡
            if start_frame is not None:
                start_timecode = FrameTimecode(timecode=start_frame, fps=fps)
            if end_frame is not None:
                end_timecode = FrameTimecode(timecode=end_frame, fps=fps)

            detect(
                video_path, 
                scene_manager, 
                start_time=start_timecode, 
                end_time=end_timecode
            )
            
            scene_list = scene_manager.get_scene_list()
            
            shots = [{
                'id': i,
                'start_frame': scene[0].get_frames(),
                'end_frame': scene[1].get_frames() - 1 # ç»“æŸå¸§-1ä»¥åŒ¹é…å¸¸è§„ç”¨æ³•
            } for i, scene in enumerate(scene_list)]
            
            return shots
        except Exception as e:
            self.logger.error(f"é•œå¤´æ£€æµ‹å¤±è´¥ for {video_path}: {e}")
            self.logger.error(traceback.format_exc()) # æ‰“å°è¯¦ç»†é”™è¯¯å †æ ˆ
            return []

    def _generate_shot_fingerprints(self, video_path: str, shot_list: List[Dict]) -> Optional[List[Dict]]:
        try:
            feats = self.load_features(video_path, 'dino');
            if feats is None: raise FileNotFoundError("DINOv2ç‰¹å¾ç¼“å­˜ä¸å­˜åœ¨")
        except Exception as e: self.logger.error(f"{e}"); return None
        return [{'shot_info': s, 'fingerprint': np.mean(feats[s['start_frame']:s['end_frame']+1], axis=0)} for s in shot_list if len(feats[s['start_frame']:s['end_frame']+1]) > 0]

    def stage3_intra_shot_alignment(self, shot_matches: List[Dict]) -> Optional[List[Dict]]:
        self.logger.info("===== é˜¶æ®µ 3: é•œå¤´å†…ç²¾ç»†å¯¹é½ =====")
        final_map = []
        for match in shot_matches:
            yt_shot, movie_shot = match['yt_shot'], match['movie_shot']
            yt_indices = np.arange(yt_shot['start_frame'], yt_shot['end_frame'] + 1)
            movie_indices = np.linspace(movie_shot['start_frame'], movie_shot['end_frame'], num=len(yt_indices))
            for i, yt_frame in enumerate(yt_indices):
                final_map.append({'yt_frame': int(yt_frame), 'movie_frame': int(round(movie_indices[i]))})
        final_map.sort(key=lambda x: x['yt_frame'])
        self.logger.info(f"ç²¾ç»†å¯¹é½å®Œæˆï¼Œç”Ÿæˆäº† {len(final_map)} ä¸ªå¸§æ˜ å°„ç‚¹ã€‚"); return final_map

    def stage4_render_video(self, movie_path: str, timeline_map: List[Dict], output_path: str) -> bool:
        self.logger.info("===== é˜¶æ®µ 4: èµ„äº§é‡ç»„ä¸æ¸²æŸ“ =====")
        if not timeline_map: self.logger.error("æ—¶é—´çº¿æ˜ å°„ä¸ºç©º"); return False
        segments = []
        if timeline_map:
            current_seg = {'start': timeline_map[0]['movie_frame'], 'end': timeline_map[0]['movie_frame']}
            for i in range(1, len(timeline_map)):
                if timeline_map[i]['movie_frame'] == current_seg['end'] + 1: current_seg['end'] = timeline_map[i]['movie_frame']
                else: segments.append(current_seg); current_seg = {'start': timeline_map[i]['movie_frame'], 'end': timeline_map[i]['movie_frame']}
            segments.append(current_seg)
        movie_info = self.detect_video_orientation(movie_path); movie_fps = movie_info['fps']
        clips_dir = self.ensure_dir(os.path.join(self.output_dir, "temp_clips"))
        clip_paths = []
        for i, seg in enumerate(segments):
            start_t = seg['start']/movie_fps; duration = (seg['end']-seg['start']+1)/movie_fps
            clip_p = os.path.join(clips_dir, f"part_{i:04d}.mp4")
            if duration <= 0.04: continue
            cmd = ["ffmpeg","-y","-ss",str(start_t),"-i",movie_path,"-t",str(duration),"-an","-c:v","libx264", "-preset", "fast", "-crf", "18", clip_p]
            subprocess.run(cmd, capture_output=True)
            if os.path.exists(clip_p): clip_paths.append(clip_p)
        if not clip_paths: self.logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•ä¸´æ—¶ç‰‡æ®µ"); return False
        list_file = os.path.join(clips_dir, "concat_list.txt")
        with open(list_file, "w") as f:
            for p in clip_paths: f.write(f"file '{os.path.abspath(p)}'\n")
        concat_cmd = ["ffmpeg","-y","-f","concat","-safe","0","-i",list_file,"-c","copy",output_path]
        result = subprocess.run(concat_cmd, capture_output=True)
        if result.returncode!=0: self.logger.error(f"è§†è§‰éƒ¨åˆ†åˆå¹¶å¤±è´¥ï¼\n{result.stderr.decode()}"); return False
        self.logger.info(f"è§†è§‰éƒ¨åˆ†æ¸²æŸ“æˆåŠŸ: {output_path}"); return True
        
    def combine_video_and_audio(self, video_path: str, audio_path: str, output_path: str) -> bool:
        self.logger.info("æ­£åœ¨åˆå¹¶æœ€ç»ˆè§†é¢‘å’ŒéŸ³é¢‘...")
        cmd = ["ffmpeg", "-y", "-i", video_path, "-i", audio_path, "-c:v", "copy", "-c:a", "aac", "-shortest", output_path]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0: self.logger.info(f"æœ€ç»ˆæˆå“ç”ŸæˆæˆåŠŸ: {output_path}"); return True
        else: self.logger.error(f"æœ€ç»ˆåˆå¹¶å¤±è´¥: {result.stderr}"); return False

    # TTSç”Ÿæˆä¸åˆæˆæ¨¡å—
    def stage5_generate_and_compose_tts_audio(self, transcript_data: List[Dict], total_duration: float, output_prefix: str, tts_params: Dict) -> Optional[str]:
        self.logger.info("===== é˜¶æ®µ 5: ç”Ÿæˆå¹¶åˆæˆTTSéŸ³è½¨ =====")
        self.load_model('tts')
        tts_model = self.models['tts']
        
        tts_parts_dir = self.ensure_dir(os.path.join(self.output_dir, "temp_tts_parts"))
        self.add_resource_for_cleanup(tts_parts_dir)
        
        tts_files_info = []
        for i, segment in enumerate(transcript_data):
            text = segment['text']; start_time = segment['start']
            if not text: continue
            self.logger.info(f"TTSç”Ÿæˆ: [{start_time:.2f}s] \"{text[:30]}...\"")
            try:
                wav = tts_model.generate(text, **tts_params)
                part_path = os.path.join(tts_parts_dir, f"part_{i:04d}.wav")
                ta.save(part_path, wav, tts_model.sr)
                tts_files_info.append({'path': part_path, 'start': start_time})
            except Exception as e: self.logger.error(f"TTSç‰‡æ®µç”Ÿæˆå¤±è´¥: {e}")

        if not tts_files_info: self.logger.error("æœªèƒ½ç”Ÿæˆä»»ä½•TTSéŸ³é¢‘ç‰‡æ®µ"); return None

        self.logger.info("ä½¿ç”¨FFmpegåˆæˆå®Œæ•´TTSéŸ³è½¨...")
        final_tts_path = os.path.join(self.output_dir, f"{output_prefix}_tts_track.wav")
        
        inputs = []; filter_complex_parts = []
        for i, info in enumerate(tts_files_info):
            inputs.extend(["-i", info['path']])
            delay_ms = int(info['start'] * 1000)
            filter_complex_parts.append(f"[{i}:a]adelay={delay_ms}|{delay_ms}[a{i}]")

        filter_complex = ";".join(filter_complex_parts)
        mix_inputs = "".join([f"[a{i}]" for i in range(len(tts_files_info))])
        filter_complex += f";{mix_inputs}amix=inputs={len(tts_files_info)}:duration=first:dropout_transition=0[out]"
        
        cmd = ["ffmpeg", "-y"] + inputs + ["-filter_complex", filter_complex, "-map", "[out]", "-t", str(total_duration), "-ar", str(tts_model.sr), final_tts_path]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode == 0 and os.path.exists(final_tts_path):
            self.logger.info(f"å®Œæ•´TTSéŸ³è½¨åˆæˆæˆåŠŸ: {final_tts_path}"); return final_tts_path
        else:
            self.logger.error(f"TTSéŸ³è½¨åˆæˆå¤±è´¥: {result.stderr}"); return None

    def process_visual_matching_for_clip(self, youtube_clip: str, movie_path: str, shot_similarity_threshold: float, sparse_interval_sec: int) -> Dict:
        self.logger.info("\n--- æ­¥éª¤ 3: è§†è§‰åŒ¹é…æµç¨‹---")
        result = {'visual_matching_status': 'failed'}
        try:
            if not self.stage0_generate_feature_bank(youtube_clip): raise ValueError("é˜¶æ®µ0å¤±è´¥(YT)")
            if not self.stage0_generate_feature_bank(movie_path): raise ValueError("é˜¶æ®µ0å¤±è´¥(Movie)")
            
            candidate_zones = self.stage1_global_coarse_localization(youtube_clip, movie_path, sparse_interval_sec)
            if not candidate_zones: raise ValueError("é˜¶æ®µ1: ç²—å®šä½å¤±è´¥")
            
            shot_matches = self.stage2_shot_matching(youtube_clip, movie_path, candidate_zones, shot_similarity_threshold)
            if not shot_matches: raise ValueError("é˜¶æ®µ2: é•œå¤´åŒ¹é…å¤±è´¥æˆ–æ— é«˜ç½®ä¿¡åº¦åŒ¹é…")
            
            timeline_map = self.stage3_intra_shot_alignment(shot_matches)
            if not timeline_map: raise ValueError("é˜¶æ®µ3: ç²¾ç»†å¯¹é½å¤±è´¥")
            
            visual_output_path = os.path.join(self.output_dir, f"visual_{os.path.splitext(os.path.basename(youtube_clip))[0]}.mp4")
            if not self.stage4_render_video(movie_path, timeline_map, visual_output_path):
                raise ValueError("é˜¶æ®µ4: è§†é¢‘æ¸²æŸ“å¤±è´¥")
            
            total_yt_shots = len(self._detect_shots(youtube_clip))
            quality_info = {
                'total_yt_shots': total_yt_shots, 'matched_shots': len(shot_matches),
                'avg_similarity': np.mean([m['similarity'] for m in shot_matches]) if shot_matches else 0,
                'confidence': (len(shot_matches) / total_yt_shots) * 100 if total_yt_shots > 0 else 0
            }
            result.update({'visual_output_path': visual_output_path, 'visual_matching_status': 'success', 'quality_info': quality_info, 'shot_matches': shot_matches})
            return result
        except Exception as e:
            self.logger.error(f"è§†è§‰åŒ¹é…æµç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}\n{traceback.format_exc()}"); result['error_message'] = str(e); return result

    # åŸå§‹çš„æ‰¹å¤„ç†æ€»æ§å‡½æ•°
    def process_csv_batch(self, csv_path: str, audio_similarity_threshold: float, shot_similarity_threshold: float, sparse_interval_sec: int, tts_params: Dict):
        self.logger.info(f"å¼€å§‹CSVæ‰¹é‡å¤„ç†ä»»åŠ¡: {csv_path}")
        entries = self.read_csv_file(csv_path)
        batch_dir = self.ensure_dir(os.path.join(self.output_dir, f"batch_run_{datetime.now().strftime('%Y%m%d_%H%M')}"))
        batch_results = []
        for entry in entries:
            task_name = f"task_{entry['index']:04d}"
            task_dir = self.ensure_dir(os.path.join(batch_dir, task_name))
            original_output_dir, self.output_dir = self.output_dir, task_dir
            
            self.logger.info(f"\n{'='*25} å¼€å§‹å¤„ç†ä»»åŠ¡: {task_name} {'='*25}")
            result = {'index': entry['index'], 'youtube_clip': entry['youtube_clip'], 'movie_path': entry['movie_path']}
            
            try:
                # 1. è§†é¢‘é¢„å¤„ç†
                self.logger.info("\n--- æ­¥éª¤ 1: è§†é¢‘é¢„å¤„ç† ---")
                yt_info = self.detect_video_orientation(entry['youtube_clip'])
                processed_yt_path = self.preprocess_video(entry['youtube_clip'], yt_info, task_name)
                
                # 2. éŸ³é¢‘åˆ†æä¸æ„è¯‘
                audio_result = self.process_single_clip(processed_yt_path, entry['movie_path'], audio_similarity_threshold, task_name)
                result.update(audio_result)
                
                # 3. è§†è§‰åŒ¹é…
                visual_result = self.process_visual_matching_for_clip(processed_yt_path, entry['movie_path'], shot_similarity_threshold, sparse_interval_sec)
                result.update(visual_result)
                
                # 4. æœ€ç»ˆåˆæˆ
                if result.get('visual_matching_status') == 'success':
                    self.logger.info("\n--- æ­¥éª¤ 4: æœ€ç»ˆåˆæˆ ---")
                    final_output_path = os.path.join(self.output_dir, f"FINAL_{task_name}.mp4")
                    
                    final_audio_path = None
                    if result.get('is_narration') and result.get('paraphrased_transcript'):
                        yt_info_full = self.detect_video_orientation(processed_yt_path)
                        total_duration = yt_info_full['total_frames'] / yt_info_full['fps']
                        final_audio_path = self.stage5_generate_and_compose_tts_audio(result['paraphrased_transcript'], total_duration, task_name, tts_params)
                    
                    if not final_audio_path:
                        self.logger.warning("æœªä½¿ç”¨TTSéŸ³è½¨ï¼Œå°†ä½¿ç”¨åŸå§‹YouTubeéŸ³é¢‘ã€‚")
                        final_audio_path = self.extract_audio_from_clip(processed_yt_path, f"{task_name}_final_audio")
                    
                    if not final_audio_path: raise ValueError("æœ€ç»ˆéŸ³é¢‘è½¨é“ä¸å¯ç”¨")
                    if self.combine_video_and_audio(result['visual_output_path'], final_audio_path, final_output_path):
                        result.update({'status': 'success', 'final_output_path': final_output_path})
                    else: raise ValueError("æœ€ç»ˆéŸ³è§†é¢‘åˆå¹¶å¤±è´¥")
                else: raise ValueError(f"è§†è§‰åŒ¹é…å¤±è´¥: {result.get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            except Exception as e:
                self.logger.error(f"å¤„ç†ä»»åŠ¡ {task_name} å¤±è´¥: {e}\n{traceback.format_exc()}"); result.update({'status': 'error', 'message': str(e)})

            batch_results.append(result)
            
            if result.get('visual_matching_status') == 'success':
                yt_full_info = self.detect_video_orientation(entry['youtube_clip']); movie_full_info = self.detect_video_orientation(entry['movie_path'])
                yt_full_info['path'] = entry['youtube_clip']
                self.save_match_report(result['shot_matches'], yt_full_info, movie_full_info, result.get('original_transcript',[]), os.path.join(task_dir, "match_report.txt"))
                self.save_quality_report(result['quality_info'], os.path.join(task_dir, "quality_report.txt"))
            with open(os.path.join(task_dir, "summary.json"), 'w', encoding='utf-8') as f: json.dump(result, f, cls=NumpyEncoder, indent=2, ensure_ascii=False)
            
            self.output_dir = original_output_dir
            self.cleanup_resources()
        
        # ä¿å­˜æ€»æŠ¥å‘Š
        summary_path = os.path.join(batch_dir, "batch_summary.json")
        with open(summary_path, 'w', encoding='utf-8') as f:
            summary_data = {'total_clips': len(entries),'processed': len(batch_results),'narration_clips': sum(1 for r in batch_results if r.get('is_narration')),'original_clips': sum(1 for r in batch_results if r.get('is_narration') is False),'success_visual_match': sum(1 for r in batch_results if r.get('visual_matching_status') == 'success'),'generated_videos': sum(1 for r in batch_results if r.get('status') == 'success'),'results': batch_results}
            json.dump(summary_data, f, indent=2, ensure_ascii=False, cls=NumpyEncoder)
        report_path = os.path.join(batch_dir, "batch_report.txt")
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"æ‰¹å¤„ç†æŠ¥å‘Š\n{'='*80}\n\n"); f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            for key, value in summary_data.items():
                if key != 'results': f.write(f"{key}: {value}\n")
            f.write("\nè¯¦ç»†ç»“æœ\n" + "-"*40 + "\n")
            for r in batch_results: f.write(f"\nç‰‡æ®µ {r['index']}:\n  çŠ¶æ€: {r.get('status', 'unknown')}\n  æ¶ˆæ¯: {r.get('message', 'N/A')}\n")
        self.logger.info(f"CSVæ‰¹é‡å¤„ç†ä»»åŠ¡å®Œæˆã€‚æ±‡æ€»æŠ¥å‘Š: {summary_path}")
        return batch_results

def main():
    # --- å…¨å±€é…ç½®---
    csv_path = "/root/input/input.csv"
    output_dir = "/root/input/output"
    
    # éŸ³é¢‘åˆ†æé˜ˆå€¼
    audio_similarity_threshold = 0.65
    
    # è§†è§‰åŒ¹é…é˜ˆå€¼
    shot_similarity_threshold = 0.82

    # ç²—å®šä½é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
    sparse_interval_sec = 2

    # TTSç”Ÿæˆå‚æ•°
    tts_params = {
        "exaggeration": 0.5,
        "cfg_weight": 0.2
    }
    
    # --- æ‰§è¡Œ ---
    matcher = EnhancedYouTubeMovieMatcher(output_dir=output_dir, log_level="INFO")
    
    results = matcher.process_csv_batch(
        csv_path=csv_path, 
        audio_similarity_threshold=audio_similarity_threshold,
        shot_similarity_threshold=shot_similarity_threshold,
        sparse_interval_sec=sparse_interval_sec,
        tts_params=tts_params
    )
    
    # --- ç»“æœæ±‡æ€» ---
    if results:
        success_count = sum(1 for r in results if r.get('status') == 'success')
        print(f"\n{'='*30}\nâœ… æ‰¹å¤„ç†å®Œæˆï¼\n{'='*30}")
        print(f"ğŸ“Š æ€»ä»»åŠ¡æ•°: {len(results)}")
        print(f"âœ¨ æˆåŠŸç”Ÿæˆè§†é¢‘: {success_count}")
        print(f"âŒ å¤±è´¥/é”™è¯¯: {len(results) - success_count}")
        print(f"ğŸ“ è¯¦ç»†è¾“å‡ºè¯·æŸ¥çœ‹ç›®å½•: {output_dir}")

if __name__ == "__main__":
    main()
